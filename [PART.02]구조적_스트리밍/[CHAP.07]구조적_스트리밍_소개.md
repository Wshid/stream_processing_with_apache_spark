# [CHAP.07] 구조적 스트리밍 소개
- **Spark SQL**의 **데이터셋 추상화**는 **유휴 데이터**를 분석하는 방법중 하나
- 본질적으로 **구조화 된 데이터**에 유용함
  - 정해진 스키마
- `Spark DataSet API`는
  - `SQL`과 유사한 **API의 표헌성**과 **스칼라 컬렉션** 및 
  - **RDD 프로그래밍 모델**을 연상시키는 `type-safe` 컬렉션 작업 결합
- 동시에 `python pandas` 및 `R dataframe`과 유사한 `Dataframe API`는
  - **함수형 패러다임**에서 주로 개발하던
  - 초기 핵심적인 **데이터 엔지니어**를 넘어, 스파크 사용자의 대상을 넓힘
- 이 높은 수준의 **추상화**는
  - 더 넓은 범위의 전문가가
  - 친숙한 API를 사용하여 빅데이터 분석 교육이 시작됨
    - 최신 데이터 엔지니어링 및 데이터 과학 실습 지원
- 데이터가 정착(settle down)될 때까지, 기다리지 않고
  - 원래 **스트림** 형태인 동안
  - 동일한 **데이터셋** 개념을 데이터에 적용할 수 있다면?
- 구조적 스트리밍 모델은 **이동 중에 데이터를 처리하기 위한** 데이터셋 SQL 지향 모델의 확장
  - 데이터가 `source stream`으로 부터 도착, 정해진 **스키마**가 있다고 가정
  - 이벤트 스트림은 **무한한 테이블**에 **추가된 행**으로 보여질 수 있음
  - 스트림에서 결과를 얻기 위해, 연산을 **해당 테이블에 대한 쿼리**로 표현
  - 동일한 쿼리를 **업데이트 테이블**에 지속적으로 적용하여
    - 처리된 이벤트의 **출력 스트림**을 생성
  - 결과 이벤트는 출력 `sink`에 제공
  - `sink`는 스토리지 시스템, 다른 스트리밍 백엔드 또는
    - 처리된 데이터를 사용할 준비가 된 app일 수 있음
- 이 모델에서 이론적으로 `unbounded table`은
  - 정의된 **제약 조건이 있는 실제 시스템**에서 구현
  - 모델 구현시, 잠재적으로 무한한 데이터 유입을 처리하기 위해
    - 특정 고려 사항 및 제한 사항이 필요
- 위 문제를 해결하기 위해 구조적 스트리밍은 아래와 같은 새로운 개념을, `DataSet/DataFrame API`에 도입
  - **이벤트 시간 지원**
  - **watermarking**
  - 과거 데이터가 얼마 오래 저장되었는지 결정하는 **다양한 출력 모드**
- 개념적으로 **구조적 스트리밍** 모델은
  - **일괄 처리**와 **스트리밍 처리**사이의 경계를 흐리게 하여
  - 빠르게 움직이는 데이터 상에서 **분석에 대한 추론 부담**을 상당히 제거

## 7.1. 구조적 스트리밍의 첫 걸음
- 간단한 `웹로그 분석` 사례를 예시로, 구조적 스트리밍 보기
- 아파치 스파크의 **기존 배치 분석**을 동일한 사례에 적용하기
- 두가지 주요 목표
  - 대부분 스트리밍 데이터 분석은 **정적 데이터 샘플**을 연구하는 것으로 시작
    - 데이터 파일 연구, 데이터가 어떻게 보이는지, 어떤 패턴을 보이는지, 직관적으로 파악
      - 그 데이터에서 의도한 지식을 추출하기 위해
      - **과정 정의**
    - 데이터 분석 잡을 정의하고, `테스트`한 이후에야
    - **데이터 분석 논리**를 이동중인 데이터에 적용할 수 있는 **스트리밍 프로세스**로 변환 가능
  - 현실적인 관점에서, 아파치 스파크가 `배치 및 스트리밍 분석`에 **균일한 API**를 사용하여
    - 배치 탐색에서 **스트리밍 어플리케이션**으로 전환하는 **여러 측면 단순화**