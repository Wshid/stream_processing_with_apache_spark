# [CHAP.10] 구조적 스트리밍 소스

## 10.1. 소스의 이해
- 구조적 스트리밍에서 **소스**
  - 스트리밍 데이터 공급자를 나타내는 **추상화**
- 스트리밍 데이터는
  - 시간이 지남에 따라 **연속적으로 발생**하는 **이벤트 흐름**
  - 단조롭게 증가하는 **카운터**로 인덱스된 **시퀀스**로 볼 수 있다는 것이
    - **소스 인터페이스**의 기본 개념
- **오프셋**은
  - 외부 소스에서 데이터를 요청하고
  - 이미 소된 데이터를 나타내는데 사용
- 구조적 스트리밍은
  - **외부 시스템**에서 현재 **오프셋**을 요청하고
  - 이를 마지막으로 처리한 **오프셋**과 비교하여 **처리할 데이터가 있는 시기**를 파악
- 처리할 데이터는 `start`, `end`사이에 **배치**를 가져와 요청
- 소스는 **주어진 오프셋**을 커밋하여 데이터를 처리했다는 정보를 받음
- 그 **소스 계약**은
  - **커밋된 오프셋**보다 작거나 같은오프셋을 가진 모든 데이터가, 처리되었으며
  - 후속 요청이 `커밋된 오프셋보다 큰 오프셋`만 규정하도록 보장
- 위 보장이 제공되면, 소스는 `처리된 데이터를 삭제`하여 **시스템 리소스**를 확보
- 오프셋 기반 처리 순서
  ```scala
  t1. getOffset = 21
  t2. getBatch(17, 21) = DataFrame(...)
  t3. commit(21)
  ```
  - t1: `getOffset`을 호출하여 `source`의 현재 오프셋을 얻음
  - t2: `getBatch(sttart, end)`를 호출하여
    - 알려진 오프셋까지의 배치를 얻음
    - 그 동안 새로운 데이터가 도착했을 수 있음
  - t3: 오프셋을 `commit`하고 `source`는 해당 레코드를 제거
- 위 프로세스는 **지속적으로 반복**하여 스트리밍 데이터 확보
- **최종 오류를 복구하기위해**
  - 오프셋은 종종 외부 저장소에 `checkpoint`됨
- 오프셋 기반 상호작용 외에도, 소스는 **두가지 요구 사항**을 충족해야 함
  - 소스를 **동일한 순서**로 재생해야 함
  - 소스는 **스키마**를 제공해야 함

### 10.1.1. 신뢰할 수 있는 소스는 지속 가능해야 함
- 구조적 스트리밍에서 **재생 가능성**(replayability)는
  - 이미 요청되었지만, 아직 커밋되지 않은 스트림의 일부를 요청
- 댜시 받고자 하는 오프셋 범위로 `getBatch`를 호출하여 이루어짐
- 소스는 `구조적 스트리밍 프로세스`가 완전히 **실패**한 후에도
  - **커밋되지 않은 오프셋 범위를 생성**할 수 있을 때, 신뢰할 수 있는 것으로 간주
- 복구 프로세스에서는
  - 마지막으로 알려진 **체크포인트**에서, `offset`이 복원되고,
  - 소스에서 다시 요청됨
- 스트리밍 프로세스 외부에 데이터를 안전하게 저장하려면
  - 소스 구현을 지원하는 **실제 스트리밍 시스템**이 필요
- 소스에서 **재생성**을 요구함으로써, 구조적 스트리밍은 소스에 **복구 책임을 위임**
- 이는 신뢰할 수 있는 소스만 **구조적 스트리밍**과 함께 작동하여
  - **강력한 end-to-end 전달 보장**을 생성함을 의미

### 10.1.2. 소스는 스키마를 제공해야 함
- **구조화된 API**의 특징
  - 서로 다른 수준에서 데이터를 처리하기 위해 **스키마**에 의존
- 불투명한 문자열 또는 바이트 배열 `blob`을 처리하는 것과 달리
  - **스키마 정보**는 **필드**와 **유형**의 관점에서 데이터가 어떻게 생성되는지를 알게 해줌
- 스키마 정보를 사용하여 **쿼리 계획**에서
  - 데이터, 스토리지 및 접근에 대한 `내부 바이너리 표현`에 이르기까지
  - 스택의 여러 수준에서 최적화 추진 가능
- 소스는 생성하는 데이터를 설명하는 **스키마 정보**를 제공해야 함
- 일부 소스 구현에서는 이 스키마를 구성하고, 이 구성정보를 활용하여
  - 수신 데이터를 자동으로 파싱, 유효한 레코드 반환 가능
- `JSON, CSV`파일과 같은 많은 **파일 기반 스트리밍 소스**가 이 모델을 따르며,
  - 이 모델에서는 사용자가 올바른 **파싱**을 위해, 파일 형식에서 사용되는 **스키마 제공**필요
- 일부 다른 소스는 모든 레코드의 **메타데이터** 정보를 표시하고
  - `payload parsing`을 app에 남겨두는 **고정된 내부 스키마**를 사용
- `스키마 중심의 스트리밍 어플리케이션`을 만드는 것은
  - 데이터가 시스템을 통과하는 방식을 `전체적으로 이해`하고
  - 다중 프로세스 스트리밍 파이프라인의 여러 단계를 공식화하기 때문에 바람직

#### 스키마 정의하기
- 스키마 정의를 생성하기 위해 `Spark SQL API`를 재사용
- 프로그래밍 방식으로 `case class`정의에서 유추하거나
  - 기존 데이터셋에 로드된 스트림의 내용을 정의하는 내용을 참고하는 등의 방법

##### 프로그래밍 방식
- `StructType`, `StructField` 클래스를 이용하여, 스키마 표현 작성
- `id, type, location coordinate`를 가진 궤도 차량을 나타내기 위해, 다음과 같이 정의 가능
- 코드
  ```scala
  val schema = StructType(
    List(
      StructField("id", StringType, true),
      StructField("type", StringType, true),
      StructField("location", StructType(List(
        StructField("latitude", DoubleType, false),
        StrcutField("longitude", DoubleType, false)
      )), false)
    )
  )
  ```
  - `StructField`는 중첩된 `StructType`을 포함할 수 있음
    - 임의의 깊이와 복잡도의 스키마 생성 가능

##### 추론을 이용한 방식
- scala에서는 스키마를 임의의 `case class` 조합을 사용하여 표현 가능
- 단일 `case class`나 `case class` 계층 구조가 제공되면
  - `case class`에 대한 `Encoder`를 작성하고
  - 해당 `encoder` instance에서 **스키마를 가져와** 스키마 표시 계산 가능
- 코드
  ```scala
  case class Coordinates(latitude: Double, longitude: Double)
  case class Vehicle(id: String, `type`:String, location: Coordinates)
  // Encoder로부터 Encoder와 schema 가져오기
  val schema = Encoders.product[Vehicle].schema
  ```

##### 데이터셋에서 추출
- 실용적인 방법: 샘플 데이터 파일을 `parquet`와 같은 **스키마 인식 형식**으로 유지하기
- 스키마 정의를 얻기 위해 **샘플 데이터셋**을 로드하고
- 로드된 `DataFrame`에서 스키마 정의를 가져옴
- 코드
  ```scala
  val sample = spark.read.parquet(...)
  val schema = sample.schema
  ```

#### 스키마 정리
- 스키마를 정의하는 **프로그래밍 방식**은 강력하지만
  - 노력이 필요하고, 유지관리가 복잡하여 오류에 이르는 경우가 있음
- `prototype`단계에서는 데이터 셋을 로드하는 것이 실용적일 수 있으나,
  - 샘플 데이터셋을 최신 상태로 유지해야할 경우, 실수로 복잡해질 수 있음
- 사례마다는 다를 수 있으나, `scala`를 사용할 때는
  - 가능하면 **추론을 이용한 방식**을 사용하는 것이 좋음

## 10.2. 사용 가능한 소스
- 파일
  - 파일로 저장된 데이터 수집 가능
  - 대부분의 경우 데이터는 스트리밍 모드에서 추가로 처리되는 레코드로 변환
  - `JSON, CSV, PARQUET, ORC` 및 일반 텍스트 형식 지원
- 카프카
  - 스트리밍 데이터 사용 가능
- 소켓
  - TCP 서버에 연결하고, **텍스트 기반의 데이터 스트림**을 사용할 수 있는 `TCP Socket Client`
  - 스트림은 `UTF8` 캐릭터셋으로 인코딩 되어야 함
- 레이트
  - 내부적으로 발생한 레코드 `(timestamp, value)`의 스트림을, 구성 가능한 **생산 속도**로 생성
  - 일반적으로 `학습 및 테스트` 목적으로 수행
- 신뢰 여부
  - 구조화된 스트리밍 프로세스가 실패하더라도, **오프셋에서 재생 기능을 제공할 때**
  - 신뢰할 수 있는(reliable)
    - 파일, 카프카
  - 신뢰할 수 없는(unreliable)
    - 소켓, 레이트
- `신뢰할 수 없는` 소스는, 데이터 손실이 용인될 수 있는 경우에만 **운영에서 사용 가능**
- 이 책에서는 `custom source`를 개발하기 위한 `public API`는 없으나
  - 앞으로 지원이 가능할 수 있음
  

## 10.3. 파일 소스
- 파일시스템의 모니터링된 **디렉터리**에서 파일을 읽음
- 파일 기반 핸드오버는
  - **배치 기반 프로세스**를 스트리밍 시스템과 연결하기 위해 일반적으로 사용하는 방법
  - 배치 프로세스는 **파일 형식**으로 출력을 생성
    - 파일 소스에 적합한 구현이, 이러한 파일을 선택하고 스트리밍 모드에서 추가 처리를 위해
    - 해당 콘텐츠를 레코드의 스트림으로 변환할 수 있는 **공통 디렉터리**에 출력을 떨굼

### 10.3.1. 파일 형식 지정하기
- `readStream` 빌더에서 `.format(<format_name>)` 메서드와 함께 제공되는 **지정된 형식**을 사용하거나
  - `DataStreamReader`에서 사용할 형식을 나타내는 **전용 메서드**를 사용하여 읽음
  - `readStream.parquet('/path/to/dir/)`과 같은 형식
- 지원되는 각 형식에 해당하는 **전용 메서드**를 사용하는 경우
  - 메서드 호출은 **빌더의 마지막 호출**로 수행해야 함

#### CODE.10.1. FileStream 구성하기
- 아래 3개의 코드는 모두 동일한 동작 수행
```scala
// 형식, 로드 경로 사용
val fileStream = spark.readStream
      .format("parquet")
      .schema(schema)
      .load("hdfs://data/exchange")

// 형식, 경로 옵션 사용
val fileStream = spark.readStream
      .format("parquet")
      .option("path", "hdfs://data/exchange")
      .schema(schema)
      .load()

// 전용 메서드 사용
val fileStream = spark.readStream
      .schema(schema)
      .parquet("hdfs://data/exchange")
```

#### spark 2.3.0에서 지원하는 파일 기반 형식
- 정적 데이터프레임, 데이터셋, `SQL API`에서 지원하는 것과 동일
  - CSV, JSON, PARQUET, ORC, TEXT, TEXTFILE

### 10.3.2. 공통 옵션
- 특정 형식과 관계없이
  - 파일 소스의 **일반적인 기능**은
  - 특정 `URL`로 식별되는 **공유 파일 시스템**에서 **디렉터리**를 모니터 하는 것
- 모든 파일 형식은 **파일 유입을 제어**하고
  - 파일의 **에이징 기준을 정의**하는 공통적인 일련의 옵션 정의

#### maxFilesPerTrigger(Default: X)
- 각 쿼리 **트리거**에서 소비될 **파일 수**를 의미
- 시스템에서 데이터 유입을 제어하는데 도움이 됨

#### latestFirst(Default: false)
- `true`, 최신 파일이 가장 먼저 처리
- 최신 데이터가 **이전 데이터**보다 우선순위가 높을 때 사용

#### maxFileAge(Default: 7days)
- 디렉터리에 있는 파일에 대한 **임계값**을 지정
- **임계값**보다 오래된 파일은 처리할 수 없으며, 효과적으로 무시됨
- 이 값은 `시스템 시계`가 아니라
  - 디렉터리의 **가장 최근 파일**과 관련된 것
- 예시
  - `maxFileAge=2days`
  - 가장 최근 파일이 **어제**인 경우
    - 파일이 오래되었다고 판별하는 임계값은 **`3days`보다 오래된 것을 의미**
- 이 특성은 **이벤트 시간**의 **워터마크**와 유사

#### fileNameOnly(Default: false)
- `true`, 두 파일의 이름이 같은 경우 동일한 것으로 간주
  - 그렇지 않을 경우 `전체 경로를 고려`

#### 유의 사항
- `latestFirst=true, maxFilesPerTrigger`가 구성될 경우
- 시스템이 최근에 발견한 파일에 **우선순위**를 부여하므로
  - 처리하기에 유효한 파일이 임곗값보다 오래된 조건이 있을 수 있기 때문에
  - `maxfileAge`는 무시됨
- 위 경우에 **에이징 정책**을 사용할 수 없음
