# [CHAP.10] 구조적 스트리밍 소스

## 10.1. 소스의 이해
- 구조적 스트리밍에서 **소스**
  - 스트리밍 데이터 공급자를 나타내는 **추상화**
- 스트리밍 데이터는
  - 시간이 지남에 따라 **연속적으로 발생**하는 **이벤트 흐름**
  - 단조롭게 증가하는 **카운터**로 인덱스된 **시퀀스**로 볼 수 있다는 것이
    - **소스 인터페이스**의 기본 개념
- **오프셋**은
  - 외부 소스에서 데이터를 요청하고
  - 이미 소된 데이터를 나타내는데 사용
- 구조적 스트리밍은
  - **외부 시스템**에서 현재 **오프셋**을 요청하고
  - 이를 마지막으로 처리한 **오프셋**과 비교하여 **처리할 데이터가 있는 시기**를 파악
- 처리할 데이터는 `start`, `end`사이에 **배치**를 가져와 요청
- 소스는 **주어진 오프셋**을 커밋하여 데이터를 처리했다는 정보를 받음
- 그 **소스 계약**은
  - **커밋된 오프셋**보다 작거나 같은오프셋을 가진 모든 데이터가, 처리되었으며
  - 후속 요청이 `커밋된 오프셋보다 큰 오프셋`만 규정하도록 보장
- 위 보장이 제공되면, 소스는 `처리된 데이터를 삭제`하여 **시스템 리소스**를 확보
- 오프셋 기반 처리 순서
  ```scala
  t1. getOffset = 21
  t2. getBatch(17, 21) = DataFrame(...)
  t3. commit(21)
  ```
  - t1: `getOffset`을 호출하여 `source`의 현재 오프셋을 얻음
  - t2: `getBatch(sttart, end)`를 호출하여
    - 알려진 오프셋까지의 배치를 얻음
    - 그 동안 새로운 데이터가 도착했을 수 있음
  - t3: 오프셋을 `commit`하고 `source`는 해당 레코드를 제거
- 위 프로세스는 **지속적으로 반복**하여 스트리밍 데이터 확보
- **최종 오류를 복구하기위해**
  - 오프셋은 종종 외부 저장소에 `checkpoint`됨
- 오프셋 기반 상호작용 외에도, 소스는 **두가지 요구 사항**을 충족해야 함
  - 소스를 **동일한 순서**로 재생해야 함
  - 소스는 **스키마**를 제공해야 함

### 10.1.1. 신뢰할 수 있는 소스는 지속 가능해야 함
- 구조적 스트리밍에서 **재생 가능성**(replayability)는
  - 이미 요청되었지만, 아직 커밋되지 않은 스트림의 일부를 요청
- 댜시 받고자 하는 오프셋 범위로 `getBatch`를 호출하여 이루어짐
- 소스는 `구조적 스트리밍 프로세스`가 완전히 **실패**한 후에도
  - **커밋되지 않은 오프셋 범위를 생성**할 수 있을 때, 신뢰할 수 있는 것으로 간주
- 복구 프로세스에서는
  - 마지막으로 알려진 **체크포인트**에서, `offset`이 복원되고,
  - 소스에서 다시 요청됨
- 스트리밍 프로세스 외부에 데이터를 안전하게 저장하려면
  - 소스 구현을 지원하는 **실제 스트리밍 시스템**이 필요
- 소스에서 **재생성**을 요구함으로써, 구조적 스트리밍은 소스에 **복구 책임을 위임**
- 이는 신뢰할 수 있는 소스만 **구조적 스트리밍**과 함께 작동하여
  - **강력한 end-to-end 전달 보장**을 생성함을 의미

### 10.1.2. 소스는 스키마를 제공해야 함
- **구조화된 API**의 특징
  - 서로 다른 수준에서 데이터를 처리하기 위해 **스키마**에 의존
- 불투명한 문자열 또는 바이트 배열 `blob`을 처리하는 것과 달리
  - **스키마 정보**는 **필드**와 **유형**의 관점에서 데이터가 어떻게 생성되는지를 알게 해줌
- 스키마 정보를 사용하여 **쿼리 계획**에서
  - 데이터, 스토리지 및 접근에 대한 `내부 바이너리 표현`에 이르기까지
  - 스택의 여러 수준에서 최적화 추진 가능
- 소스는 생성하는 데이터를 설명하는 **스키마 정보**를 제공해야 함
- 일부 소스 구현에서는 이 스키마를 구성하고, 이 구성정보를 활용하여
  - 수신 데이터를 자동으로 파싱, 유효한 레코드 반환 가능
- `JSON, CSV`파일과 같은 많은 **파일 기반 스트리밍 소스**가 이 모델을 따르며,
  - 이 모델에서는 사용자가 올바른 **파싱**을 위해, 파일 형식에서 사용되는 **스키마 제공**필요
- 일부 다른 소스는 모든 레코드의 **메타데이터** 정보를 표시하고
  - `payload parsing`을 app에 남겨두는 **고정된 내부 스키마**를 사용
- `스키마 중심의 스트리밍 어플리케이션`을 만드는 것은
  - 데이터가 시스템을 통과하는 방식을 `전체적으로 이해`하고
  - 다중 프로세스 스트리밍 파이프라인의 여러 단계를 공식화하기 때문에 바람직

#### 스키마 정의하기
- 스키마 정의를 생성하기 위해 `Spark SQL API`를 재사용
- 프로그래밍 방식으로 `case class`정의에서 유추하거나
  - 기존 데이터셋에 로드된 스트림의 내용을 정의하는 내용을 참고하는 등의 방법

##### 프로그래밍 방식
- `StructType`, `StructField` 클래스를 이용하여, 스키마 표현 작성
- `id, type, location coordinate`를 가진 궤도 차량을 나타내기 위해, 다음과 같이 정의 가능
- 코드
  ```scala
  val schema = StructType(
    List(
      StructField("id", StringType, true),
      StructField("type", StringType, true),
      StructField("location", StructType(List(
        StructField("latitude", DoubleType, false),
        StrcutField("longitude", DoubleType, false)
      )), false)
    )
  )
  ```
  - `StructField`는 중첩된 `StructType`을 포함할 수 있음
    - 임의의 깊이와 복잡도의 스키마 생성 가능

##### 추론을 이용한 방식
- scala에서는 스키마를 임의의 `case class` 조합을 사용하여 표현 가능
- 단일 `case class`나 `case class` 계층 구조가 제공되면
  - `case class`에 대한 `Encoder`를 작성하고
  - 해당 `encoder` instance에서 **스키마를 가져와** 스키마 표시 계산 가능
- 코드
  ```scala
  case class Coordinates(latitude: Double, longitude: Double)
  case class Vehicle(id: String, `type`:String, location: Coordinates)
  // Encoder로부터 Encoder와 schema 가져오기
  val schema = Encoders.product[Vehicle].schema
  ```

##### 데이터셋에서 추출
- 실용적인 방법: 샘플 데이터 파일을 `parquet`와 같은 **스키마 인식 형식**으로 유지하기
- 스키마 정의를 얻기 위해 **샘플 데이터셋**을 로드하고
- 로드된 `DataFrame`에서 스키마 정의를 가져옴
- 코드
  ```scala
  val sample = spark.read.parquet(...)
  val schema = sample.schema
  ```

#### 스키마 정리
- 스키마를 정의하는 **프로그래밍 방식**은 강력하지만
  - 노력이 필요하고, 유지관리가 복잡하여 오류에 이르는 경우가 있음
- `prototype`단계에서는 데이터 셋을 로드하는 것이 실용적일 수 있으나,
  - 샘플 데이터셋을 최신 상태로 유지해야할 경우, 실수로 복잡해질 수 있음
- 사례마다는 다를 수 있으나, `scala`를 사용할 때는
  - 가능하면 **추론을 이용한 방식**을 사용하는 것이 좋음